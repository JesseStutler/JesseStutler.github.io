<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Jesse's Blog</title><link>https://jessestutler.github.io/posts/</link><description>Recent content in Posts on Jesse's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>本站遵循 CC-BY-NC 4.0 协议</copyright><lastBuildDate>Mon, 21 Jun 2021 21:45:58 +0800</lastBuildDate><atom:link href="https://jessestutler.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>K8s Controller概析</title><link>https://jessestutler.github.io/posts/k8s-controller%E6%A6%82%E6%9E%90/</link><pubDate>Mon, 21 Jun 2021 21:45:58 +0800</pubDate><guid>https://jessestutler.github.io/posts/k8s-controller%E6%A6%82%E6%9E%90/</guid><description>k8s-controller 本篇文章介绍k8s-controller的大致结构和大致流程
此图展现了Controller的整体通用结构，上部Client-go部分是所有controller通用代码的抽象包（包括各种资源的controller，kube-proxy等，这部分大家都差不多），而下部custom controller是具体controller的逻辑代码
1）Reflector list&amp;amp;watch controller自己有对监控的资源的缓存，与从api server中查到的数据（最终来自etcd）一致，List用来获取资源的全量数据（每经过resyncPeriod时间，就使用List获取最新版本的API对象，强制更新缓存，保证缓存的有效性），Watch用来对资源进行监听，以增量的方式接收资源对象的变更。Reflector会保存一个lastSyncResourceVersion，表示上次同步的版本，用来进行容错，出错时就从上次同步的版本开始。Reflector的工作就是将从api server HTTP GET到的数据实例化成对象，可以把Reflector视为informer的前端
Reflector工作流程：
从api server获取一次全量对象，更新ResourceVersion，插入一个Sync类型的Delta到Delta FIFO中 开一个go routine每resyncPeriod获取全量对象，做法同1 Watch ResourceVersion以后的对象变化情况，一旦有对象变化，根据变化类型（是新增，更新还是删除）产生相应类型的Delta插入到DeltaFIFO中 2) Reflector Add Object Reflector作为生产者，informer作为消费者，Reflector将资源的变更事件放到队列当中，由Informer处理增量事件（如add，update，delete等）
增量事件表示（Delta）：
type Delta struct { Type DeltaType // 增量事件类型，如Added, Updated, Deleted, Sync Object interface{} //资源实例全部信息 } 3）Informer Pop object &amp;amp; 4) Informer Add Object &amp;amp; 5) Indexer Store Object informer从delta队列中取得资源对象实例，更新本地缓存，Indexer实际上就是一个拥有读写锁的且拥有索引机制快速查找的Map，用来缓存监控资源对象数据
type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} indexers Indexers indices Indices } 6) Informer Dispatch Event Handler Functions Informer根据增量事件的类型交给初始化informer时注册的handler处理事件</description></item><item><title>K8s Iptables分析</title><link>https://jessestutler.github.io/posts/k8s-iptables%E5%88%86%E6%9E%90/</link><pubDate>Thu, 10 Jun 2021 14:51:48 +0800</pubDate><guid>https://jessestutler.github.io/posts/k8s-iptables%E5%88%86%E6%9E%90/</guid><description>K8s iptables分析 笔者最近因为需要看了下k8s的kube-proxy在默认iptables模式下对于iptables规则是如何管理的，以此作为记录并分享，后面会继续跟进kube-proxy并进行源码分析
这里笔者分享一套还不错的iptables学习路线：
https://www.zsythink.net/archives/category/%e8%bf%90%e7%bb%b4%e7%9b%b8%e5%85%b3/iptables
iptables知识 查 -t [表名] 指定想查看的表的规则，默认为filter表
-L [链名] 指定想查看的某个表在某个链当中的规则，不写链列出表在所有链当中的规则
-v 列出详细规则
-n 不做域名解析，直接显示ip（通常-nvL同行）
&amp;ndash;line-number 显示行数
e.g:
iptables -t nat -L INPUT
列出nat表在INPUT链中的所有规则
增 -I [链名] 插入规则到指定的链当中（在链的首部插入）
-I [链名] [序号] 插入规则到指定的链当中（指定是链的第几条规则）
-A [链名] 插入规则到指定的链当中（在链的尾部插入）
-s [源地址,[源地址]&amp;hellip;] 或[源地址网段]
-d [目标地址]
用!在-s或-d前表示取反，非地址匹配
&amp;ndash;sport 源端口
&amp;ndash;dport 目标端口
&amp;ndash;sport/dport [起始端口:结束端口] 表示匹配此范围内端口
指定源端口和目标端口为扩展匹配条件，-m用来指定扩展模块，如-m tcp -dport 22表示匹配ssh，若不指定-m（最好指定），默认与-p指定的协议相同
-m [模块] 使用模块扩展匹配条件
离散端口： -m multiport --dports 22,53,80 需要指定multiport模块用来离散匹配 范围ip： -m iprange --src-range 192.</description></item><item><title>NSDI17 ExCamera</title><link>https://jessestutler.github.io/posts/nsdi17-excamera/</link><pubDate>Tue, 18 May 2021 14:45:44 +0800</pubDate><guid>https://jessestutler.github.io/posts/nsdi17-excamera/</guid><description>NSDI17-ExCamera 这是一篇将video encoding改造到serverless平台上的文章
论文链接：https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/fouladi
mu框架 大致流程 AWS s3 invoke第一个Worker（function实例），然后Worker与Coordinator建立TLS连接并保持（Coordinator通过RPC call来调控Worker的状态，Worker就是一个有限状态机），当Coordinator收到来自Worker的message时，就会根据状态转换逻辑产生新的状态给Worker并发送下一个RPC请求。
Coorinator是dependency-aware的，他会根据Worker产生的output来指派可以处理这个output的worker，这样就可以顺序执行而不会产生死锁
出现原因 传统的视频encoding速度太慢，一些实时的视频处理平台需要快速的视频上传业务
背景知识：我们都知道视频由一帧帧的图片组成，对于将一段视频压缩成比特流来说，有些帧与帧之间，图片的某些部分是重复的，那压缩成比特流就不必重复，encoding过程就是花费cpu时间来寻找帧与帧之间的联系，从而尽可能的压缩输出的比特流大小；但是这样带来的问题就是帧与帧之间会存在依赖关系，从而不能将比特流从中间段进行解码，比如说直播的时候有不同的清晰度，想要切换成更高清的流。现在引入Stream Access Point来切分视频数据流
借助Stream Access Point技术（将视频数据流进行切分，各段数据流都是独立的，段与段之间的帧没有依赖关系，VP8/VP9使用的是 &amp;ldquo;key frame&amp;quot;概念），可以将各段encoding过程改造成使用相同的function来处理，各段压缩完成之后再进行简单的连接（串行），形成一个完整的视频数据流
ExCamera encoding流程 （并行）使用vpxenc（谷歌优化的encoder）encode六个帧，都以key frame为开头（也就是使用Stream Access Point分割视频数据流）,这代表一个chunk
（并行）使用ExCamera设计的encoder将原先vpxenc生成的key frame替换成与前面部分的encoder产生的输出相关联的inter frame（因为key frame会影响压缩速率，所以ExCamera针对此进行了优化）。最后生成的chunk只有一个key frame为开头。
这step2与step3之间还涉及到很多并行优化步骤，因为涉及到视频encode和decode背景，略过
（串行）将各个chunk顺序连接起来</description></item><item><title>ATC20——Faasm</title><link>https://jessestutler.github.io/posts/atc20faasm/</link><pubDate>Tue, 18 May 2021 14:43:26 +0800</pubDate><guid>https://jessestutler.github.io/posts/atc20faasm/</guid><description>ATC20——Faasm 这是一篇关于webassembly sandbox的文章
论文链接：https://www.usenix.org/conference/atc20/presentation/shillaker
为什么要提出WASM-sandbox（本文是Faaslet)？ 大多数serverless平台使用的是容器承载function，但是对于容器来说，启动开销和过多的memory footprint仍然与serverless场景不太匹配（像边缘场景如果容器是overprovision的，性能会随着资源可用量的减少而下降；而且边缘如果是多租户的，long-running container也不合需求，如果资源不够用了需要频繁的驱逐），而且现有以容器为承载的方案（尽管有提出本地存储来减少访问数据开销的）会产生冗余数据，每个函数都有一份拷贝，而且需要重复的序列化和网络开销。对比docker来说，faaslet能够极大的减少冷启动延迟，减少开销，让一台机器承载更多的sandbox
Fasslet function和其library，runtime都会编译为WASM；
cgroup做cpu周期隔离；
network namespace做Network隔离和提供virtual network interface；
faaslet以线程运行，共享进程资源；
部分WASI+部分POSIX实现（图中Host interface）做system calls，因为WASI是基于compability-based security的，所以对于资源的访问是通过不可伪造的句柄来保存引用的
两层状态共享机制 local状态共享就是多个faaslet（多个线程）共享父进程的同一块内存区域，global负责集群内状态的同步
faaslet快照 预初始化faaslet并做成快照，可以减少冷启动时间和各种开销</description></item><item><title>ATC18——容器优化方案SOCK</title><link>https://jessestutler.github.io/posts/atc18%E5%AE%B9%E5%99%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88sock/</link><pubDate>Tue, 18 May 2021 14:41:22 +0800</pubDate><guid>https://jessestutler.github.io/posts/atc18%E5%AE%B9%E5%99%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88sock/</guid><description>ATC18——容器优化方案SOCK 这是一篇优化容器冷启动的文章
论文链接：https://www.usenix.org/conference/atc18/presentation/oakes
解构container（docker瓶颈） Bind mount可能比AUFS（或overlay）性能更好 频繁的container创建和删除（涉及到频繁的namespace的创建和删除，可能会有性能瓶颈，比如network namespace，并发的creation和cleanup越多延迟越高），是不是可以把一些不必要的namespace隔离给剔除或者进行一些优化（disable创建或删除时不必要的影响性能的功能） 频繁的创建和删除cgroup不如reuse cgroup，比如维护一个初始化好的cgroup池 当host上挂载的越多，mount namespace拷贝的速度就越慢，简单的做法可以考虑使用chroot SOCK优化方案 Lean containers:
用bind mount代替overlay，分四层：系统层（base），package层（read-only，用来package caching），code层（lambda代码），scratch层（就是container layer，可写层） 用cgroup pool来分配在container创建时分配cgroup，container删除时重新回到池中 将mount namespace和network namespace省去，其瓶颈在docker瓶颈中已提到 Zygote机制：
Zygote container就是一些已经预import需要的package的容器，内含Zygote进程，这样从这个进程fork出的新进程（子进程）并创建出的新容器不需要做重复性的初始化工作，直接从内存读相同的内容就好了，也就是：
含Zygote进程的container&amp;ndash;&amp;gt;含从Zygote fork出的子进程的container
三级缓存：
handler cache：
将idle instance pause，不消耗cpu但是消耗内存，之后再有request过来unpause是比新创建一个container快的（warm start）
install cache：
lean containers中的package层，read-only且被所有container共享
import cache:
就是Zygote机制，但是命中和驱逐机制需要定制。命中可能与传统cache不同，存在多命中的情况（handler需要的包可能既在tree cache中的子节点也可能是父节点），这时需要找到最合适的entry（也就是Zygote进程）；驱逐因为Zygote进程会都有相同的包而存在共享内存的情况所以比较复杂</description></item><item><title>ATC18——窥探Serverless平台</title><link>https://jessestutler.github.io/posts/atc18%E7%AA%A5%E6%8E%A2serverless%E5%B9%B3%E5%8F%B0/</link><pubDate>Tue, 18 May 2021 14:31:18 +0800</pubDate><guid>https://jessestutler.github.io/posts/atc18%E7%AA%A5%E6%8E%A2serverless%E5%B9%B3%E5%8F%B0/</guid><description>ATC18——窥探serverless平台 这是一篇利用逆向工程测试Serverless平台的文章
论文链接：https://www.usenix.org/conference/atc18/presentation/wang-liang
QA Q:隔离的减少会导致I/O，networking，coldstart等表现的下降？
A:是的。如果VM中有多个instance实例会造成资源争夺的现象（见衡量指标中的I/O &amp;amp; network throughput）
Q:同一VM是运行多个function instance吗？
A：是。AWS可以通过I/O测试发现多个function instance共享/proc中的文件
Q:不同租户的function实例可以放到同一VM里吗？
A：可以但并没有平台采用（安全隔离性会有问题:side channel attack）
Q:idle instance（暂无请求的实例，但是不会收用户费用）是要退出并收回资源还是再利用（先放到池里）处理后续的请求？
https://aws.amazon.com/cn/blogs/compute/container-reuse-in-lambda/
A:这个问题值得考量。一方面，idle instance会一直占用VM的资源；而另一方面，如果有突发的请求又可以减少instance的冷启动时间。所以折中来说，AWS采用的是将一个函数的一半的instance每300s停掉并回收资源，剩下的instance运行直到一个最大idle time为止。
Q:多个request会被同个instance接收吗？
A：会。Google针对负载过多的话会开新实例
Q:function update是开新实例吗，还是在旧实例的基础上改？
A：开新实例，负载会从旧实例慢慢过渡到新实例，但是有一个时间差
衡量指标 Cold-start latency（&amp;amp; warm start latency）
这里代表的是function的冷启动时间（AWS使用了VM池在function启动之前就准备接收function的调度，这样基本上只受scheduling latency的影响）
warm start指function在执行完之后，暂时“冻住”，为不久后再有请求而“解冻”并处理
Function instance lifetime
即使instance仍然在运行，但是到达一个lifetime也会被terminated，租户如果想用一个function维护in-memory state的话肯定想让这个instance运行地更久一点
AWS instance lifetime中位数为6.2小时
Maximum idle time before shut down
I/O &amp;amp; network throughput
当VM中的function实例越多，每个function的I/O和network吞吐量会越小，而且会受到function分配到的内存的影响，function占用的内存越大，吞吐量越高。所以，存在一个VM多instance的资源争用问题
CPU usage（AWS是根据code的预配置memory量来分配cpu周期，memory量越多CPU周期越多，这样冷启动的时间也会减少越多，而且公平）
Memory usage</description></item><item><title>ATC18——高性能workflowSAND</title><link>https://jessestutler.github.io/posts/atc18%E9%AB%98%E6%80%A7%E8%83%BDworkflowsand/</link><pubDate>Tue, 18 May 2021 14:28:02 +0800</pubDate><guid>https://jessestutler.github.io/posts/atc18%E9%AB%98%E6%80%A7%E8%83%BDworkflowsand/</guid><description>ATC18——高性能workflow SAND 这是一篇关于优化serverless workflow的文章
论文下载链接：https://www.usenix.org/conference/atc18/presentation/akkus
Sandbox 同属于同一个workflow的function属于一个application，一个application一个container，而不是一个function一个container
但是有可能会引入资源竞争问题？
当有request到来时，==通过fork function实例来快速水平扩展==（实验证明，fork是最快的，比直接执行二进制文件创建进程都要快），而不是频繁的冷启动一个不同的container。而且，同一个function的不同实例（也就是进程）可以共享内存，库只要加载一次就够了，相比另起一个container的内存占用量，内存占用量少很多。function执行完之后可以回收资源，等有请求来了再fork新实例，相比为了解决负载尖峰而一直保持container idle占用资源，可以避免资源一直被占用。
Message Bus SAND使用了一个机制：同一台host中的function通过local message bus来沟通，不同host中的function通过global message bus来沟通，而且global message bus可以保存local message bus中的消息作为备份（用来容错）
tips：local/global message bus都为不同的funciton维护有不同的队列（或者topic），global像kafka这种实现有partition做容错，host agent订阅global message bus，本地function订阅local message bus。而且message bus不直接传递data，而是传递数据的引用（比如local可以通过in-memory的key-value存储，来快速获取数据，global可以通过分布式存储来获取数据），不仅存取数据快，这样检查状态和回滚也方便。
鉴于现有的serverless平台中的workflow沟通机制，即使两个function在同一台机子当中，也是要通过外部消息队列服务来存取的，这引入了极大的延迟，local message bus能够削减这段延迟时间
Host Agent Host agent是每台机子上的代理，他负责local message bus和global message bus的合作（比如备份，细节里会细说）；为自己机子上的函数从global message bus存取消息（订阅topic）；孵化容器和fork function
细节 如何做备份 当function产生message到local message bus当中自己的队列时，会产生一份拷贝给host agent的队列，然后host agent将这份拷贝消息放到global message bus的这个function的队列（或者topic）当中，==作为备份并且打上标签表示完成状态==，host agent会追踪要接收这个消息的下一个function的完成进度，顺利完成会将状态转为finished，处理失败会将状态转为failed并交给另一个机子上的function处理
workflow流程 假设有两个function完成workflow，一台机子（两个partiton)
Step1：user request发送给function1，global message bus将消息放到partition1</description></item><item><title>Serverless分析</title><link>https://jessestutler.github.io/posts/serverless%E5%88%86%E6%9E%90/</link><pubDate>Tue, 18 May 2021 14:13:36 +0800</pubDate><guid>https://jessestutler.github.io/posts/serverless%E5%88%86%E6%9E%90/</guid><description>Serverless分析 本文根据Berkeley rise lab的综述Cloud Programming Simplifified:
A Berkeley View on Serverless Computing并结合其他相关材料进行总结，探究serverless的研究点，本文会持续进行更新。
简单的说，Serverless就是FaaS+BaaS
特点 按使用量付费（无请求时无资源无分配无花费，有请求时按使用量，按时间计算付费），性能提高（高并发量），autoscale，强隔离性（多租户），可供有突发流量情况而又无服务器扩展需求的公司使用；
低请求量服务改造：原先需要一直监听请求的应用，当无请求来时需要一直占用资源，而改造成serverless可以用function代替原先的应用，这样无请求来临时可以down to zero，有请求来时再invoke一个或多个function实例（而且这些function是可以并行的）并进行处理；（不仅是针对可以减少资源使用量，而且可以应对流量尖峰）
由外部服务触发比如S3（有object更新，比如新增图片），消息队列（事件驱动，收到事件），或者以API gateway的形式（可以是以Backend或以function的形式）等待HTTP request到来触发
一定是stateless，无法保证写到memory或者local disk的数据（VM上）下次被invoked还能读到，需要借助外部存储服务来保存状态或数据
适合short-lived task
从serverful过渡到serverless就像从汇编语言过渡到高级语言一样，汇编语言计算一个c=a+b需要指定寄存器，存放，计算结果然后并存回，而serverful就像汇编语言一样需要先知道哪些资源是可用的，然后给资源加载code和环境，执行计算，再得到结果，这些原先需要平台使用者去知晓，但是serverless不需要programmer去知晓和管理资源，只需要编写code，编写function，编写业务就够了
现今Serverless的有限性 存储对于细粒度操作的局限性 因为function之间是相互隔离的，所以需要借助外部存储服务(BaaS)来提供状态的支持，这是serverless的特性所致。但是对于划分到function这么细粒度的操作来说，现在的外部存储服务要不是太贵（access或者storage）要不就是延迟太高，e.g:对象存储比如AWS S3等，access花费和延迟过高；key-value数据库存储费用高，扩容慢；内存存储如redis等没有容错性，不能自动扩缩。当然这要看应用的要求，但还是与serverless理想的存储方案相差不少。
缺少细粒度的消息沟通 背景：两个task合作，taskA需要taskB的output作为input，但是不知道何时output会过来，所以需要引入消息中间件，但是现有的消息中间件对于细粒度(task/function)操作的延迟和花费太高
可能的解决方案：自己设计消息通知机制比如长期运行一个汇集消息的server，能够以命名的方式直接定位到function实例从而获取到ouput等
标准沟通模式对于细粒度的性能太差 背景：broadcast，shuffle，aggregation都是分布式系统中重要的原语，但是如果划分粒度过细，比如拿聚合来说，VM实例中的function如果本地不做聚合而每次聚合都需要到远端聚合，那么这个消息数量会成倍增加，shuffle则更多
冷启动的局限性 1）启动function需要一定时间（分配和加载资源：分配VM，初始化container，将function的静态文件拷贝到container）
2）需要一些时间去下载函数执行环境（OS，库，语言的runtime比如JVM等）
函数的package依赖需要经过远端的download，local install，import过程，这个时间比较长，是否可以在本地machine上预先下载好所有语言涉及的包？（通过压缩的方式存储）这样直接去本地加载package，省去去远端下载package的时间。所有container通过overlayfs或者bind mount共享已经安装好的package
SOCK：利用Zygote机制预import一些需要的package（这样的Zygote很多，需要预import什么package就fork出新的Zygote），这样从Zygote进程fork出的新子进程不需要进行同样的初始化操作，直接从内存读取即可（减少开辟新内存的消耗）
tips：fork出的子进程与父进程共享堆栈，fd，代码段，由于copy-on-write，只有子进程写时才会完全拷贝
含Zygote进程的container&amp;ndash;&amp;gt;含从Zygote fork出的子进程的container
3）有些应用对于代码需要做一些定制的初始化操作，需要花费一定时间（比如加载和初始化数据结构，库等）
4）如果需要频繁冷启动，namespace的频繁creation和cleanup需要性能损耗
什么时候冷启动会发生？ 当function的code或者配置改变的时候，或者function第一次部署的时候 idle instance被shut down instance到了最大age被shut down（即使仍然在运行） 之前的instance都在忙于处理请求，需要横向扩展的时候 什么时候需要考虑冷启动的影响？ 也许像要访问存储服务的function本来就需要等待存取的latency，冷启动时间相对这段latency可有可无；也许实时数据流服务会频繁地invoke function，function会一直处理event很多次（可能200000次在到达最大age之前），那冷启动时间也可有可无。</description></item><item><title>Raft</title><link>https://jessestutler.github.io/posts/raft/</link><pubDate>Tue, 09 Mar 2021 16:30:52 +0800</pubDate><guid>https://jessestutler.github.io/posts/raft/</guid><description>Raft 引言 Raft是分布式数据一致性算法，用于解决PAXOS多年来晦涩难懂且难以工程复现的问题，本文对Raft发表的原文论文进行了大致解读
基本算法内容 Basics Follower,Candidate,Leader 每个server分为三种状态（状态转换图见Leader election）：
Follower：只接受RPC请求（就算收到来自client的请求也会重定向给leader） Candidate：参加竞选，可以发送RequestVote RPC，同样也可以接受请求 Leader（only one）：只有Leader可以处理来自client的请求，可以发送AppendEntries RPC，可以是追加日志条目用，也可以是心跳检测用（定期检测其他server是否还活着，通过无条目追加的AppendEntries RPC来做到） term——任期 Raft将时间随机划分，每一段称为任期（任期是单调递增的），任期都以一次选举开始，选举可以是选出leader也可以是未能选出leader（未能选出leader就直接进入下一任期）
每台server发现自己的任期小于其他机器就需要update到最新
entry——条目 条目就是指日志的条目，由client发来的command+任期数（term number，用来检测不一致性）+index（条目索引）构成
Leader election 状态转换图分析：
Starts up:
初始时，每台server都是Follower
Follower&amp;mdash;&amp;gt;Candidate：
当超出election timeout时间（长时间未收到有Leader发过来的RPC消息，说明当前cluster未选出leader，或者是未收到来自candidate的RequestVote RPC），Follower增加自己的当前任期数，并将自己转换为Candidate；参加竞选leader，给自己投票，然后并发地向其他server发送RequestVote RPC请求，需要他们给自己进行投票（一般规则是先收到谁请求就投谁）；重设election timeout
Candidate&amp;mdash;&amp;gt;Candidate：
选举发生投票分歧 未能选出leader（发生投票分歧），比如有好几台Candidates票数一致的情况，或者大家都是Candidate（不可能给竞争对手投票是吧:P ），增加自己的当前任期数，并开始新一轮的选举。不过这样有可能造成一直产生投票分歧的情况，打破这种情况并选出leader的机制就是election timeout，Candidates从时间段中随机给自己选一个election timeout时间，如果发生投票分歧，先超时的Candidate赢得选举
Candidate收到Leader（已经暂时选出的）的RPC请求，发现其任期比自己旧，拒绝请求并保持Candidate状态 Candidate&amp;mdash;&amp;gt;Leader：
赢得竞选（获得大多数servers的投票）
Leader&amp;mdash;&amp;gt;Follower:
通过RPC的回复发现自己的任期已过期（有比自己更新的任期），退回到Follower
Candidate&amp;mdash;&amp;gt;Follower：
输掉选举（收到了来自己已选出的leader的RPC，但要确定自己的任期至少和Leader的任期相同，参考第3步） 通过RPC的回复发现自己的任期已过期（有比自己更新的任期），退回到Follower Log Replication 首先要说明的是，Leader只追加条目（entry）而不修改或删除entry</description></item><item><title>Golang基础</title><link>https://jessestutler.github.io/posts/golang%E5%9F%BA%E7%A1%80/</link><pubDate>Sun, 07 Mar 2021 11:25:55 +0800</pubDate><guid>https://jessestutler.github.io/posts/golang%E5%9F%BA%E7%A1%80/</guid><description>GOLANG 引言 云原生体系下，golang一定是必须要掌握的高级语言，golang内置的goroutine契合分布式架构的设计，越来越多的关于云的开源项目采用go进行实现。本文参考：http://c.biancheng.net/golang/intro/ ，本文对go的基础知识进行了大致介绍，可参照右侧的大纲检索，本文会持续更新。
这里笔者也推荐一些go练手项目：
https://geektutu.com/post/gee.html 极客兔兔的7天用go从零实现系列（必须强烈推荐👍，一天天打下来能对很多go的开源项目有个大致的认识，也能学到很多技巧） https://courses.calhoun.io/courses 需要挂梯子，作为一些对于go不同的包的练手项目不错 常用命令 go install [package-name]
编译并安装包，如果不是main包则会安装到pkg底下作为库包，如果是main包则会安装到bin底下作为可执行文件
go doc [package] [func]
go手册
go build [file or package]
如果是main包，生成可执行文件（可执行文件名同文件夹名），如果不是main包，不生成可执行文件，只进行编译
如果是单个.go文件，main包中只能对含main函数的go文件进行编译并生成可执行文件，其他包只进行编译
go module 以后默认用go module的方式进行包管理和添加依赖，以后workspace不必在$GOPATH下，$GOPATH就存放下载的包和编译好的课执行文件
root workspace底下有两个文件，一个是go.mod（包管理），一个是go.sum（包校验），真正的包下载好放在$GOPATH/pkg/mod底下
在root workspace底下用go mod init[模块名]生成go.mod，模块名命名格式为example.com/xxx..（一般就github.com/foo这样），然后引用workspace底下子目录的go文件，用模块名/子目录名的格式引用
给项目添加依赖（写进 go.mod)的两种方法：
你只要在项目中有 import，然后 go build 就会 go module 就会自动下载并添加（perfect way） 自己手工使用 go get 下载安装后，会自动写入 go.mod 常用命令 go mod init</description></item><item><title>Docker底层原理</title><link>https://jessestutler.github.io/posts/docker%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</link><pubDate>Sun, 07 Mar 2021 11:12:39 +0800</pubDate><guid>https://jessestutler.github.io/posts/docker%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</guid><description>docker底层原理 namespace——资源隔离 namespace 是 Linux 内核用来隔离内核资源的方式，同一个namespace中的进程之间可以互相感知，不同namespace之间的进程是相互独立的，docker本身就是一个进程，通过namespace来实现隔离，从而模拟独立运行环境，在/proc/$$/ns下能查看当前进程下的所有link文件，每个link文件对应不同的namespace，如果不同的进程间有相同的namespace的inode号，则他们是共享namespace的，否则他们属于不同的的namespace
**通过clone()函数在创建子进程的同时给子进程创建新的namespace（传入CLONE_*宏定义）**
UTS namespace：
提供主机名和域名的隔离，使容器能够通过服务名访问
IPC namespace（进程间通信）：
实现信号量、消息队列和共享内存等资源的隔离
PID namespace：
对进程内的PID重新标号，从1开始，每个PID namespace都有自己的计数程序，宿主机的PID namespace相当于创建进程的PID namespace的parent pid namespace，能看到子节点（child pid namespace)中的内容，但子节点不能看到父节点当中的内容，这样父节点能在外部管理容器内的进程
如果pid namespace中的某个进程的父进程被杀死，该进程成为孤儿进程，则会被当前pid namespace的init进程（pid为1，如/bin/bash）收养，成为其子进程
mount namespace:
通过隔离文件系统挂载点来隔离文件系统（当创建新的mount namespace时，会将所有挂载点复制给子进程，但在这之后，子进程对自己namespace内文件系统进行的操作不会影响到父进程namespace）
可以通过共享挂载机制传播挂载（主从挂载、共享挂载等）
network namespace:
提供网络资源的隔离，包括网络设备、协议栈、路由表、防火墙等等
默认的bridge模式：
每个容器有独立的network namespace，宿主机通过docker0网桥（虚拟网桥）来连接不同的network namespace，容器通过veth pair（虚拟以太网端口对，它们组成了一个数据的通道，数据从一个设备进入，就会从另一个设备出来）连接docker0网桥，设备的一端放在新创建的容器中，并命名为eth0。另一端放在主机中，以veth65f9这样类似的名字命名。
如果容器想主动和外界通信，或者外界想访问容器内的服务（访问宿主机的端口），实际上这是通过iptables来管理的（进行了转发和NAT转换等操作）
host模式：
容器和宿主机共享network namespace，但其他namespace与宿主机隔离，容器用的是宿主机的ip与外界通信，性能较好但易产生端口冲突
container模式：
新创建的容器若指定container模式，则和已经存在的容器共享一个network namespace，与此容器共享ip和协议栈
user namespace
提供安全隔离，比如用户id，用户组，权限等，在子进程的user namespace中拥有新的用户和用户组，在父进程中的普通用户可能却成为子进程中namespace的超级用户，结构与pid namespace类似（树状结构），子user namespace中的用户和用户组需要与父user namespace中的用户和用户组相对应（做映射），这样这个user namespace才能与其他user namespace中的进程通信，甚至访问共享的文件（即对应到其他user namespace的用户和用户组并拥有相应的权限，如果没有相应的权限就不能在其他user namespace执行某些操作）</description></item><item><title>Docker基础知识</title><link>https://jessestutler.github.io/posts/docker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Sun, 07 Mar 2021 10:56:15 +0800</pubDate><guid>https://jessestutler.github.io/posts/docker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>Docker Docker拉取镜像流程图 Docker CLI 镜像命令 docker images 查看本地的镜像
docker images [image-name[:tag]] 默认不加参数就是-a，或者指定image的名字，可在image之上再加版本号 &amp;ndash;a 列出所有镜像 &amp;ndash;q [image] 列出镜像的id（-aq是列出所有的镜像id）
docker search 镜像
搜索远程仓库镜像（docker hub上查看更详细）
docker rmi [repo[:tag]]
删除本地镜像，使用方法与images相同，注意rmi是删除镜像，rm是删除容器
或者根据docker images -q [image]列出的id进行删除
docker tag source_image[:tag] target_image[:tag]
改镜像标签名（不然默认传到docker hub上的library仓库会被拒绝）
docker pull 镜像
默认拉取的是docker hub上的Image，也可以用一个容器跑一个local docker registry，然后让其他使用了docker pull并指定了docker registry地址和端口的机子从这台运行了docker registry的机子上拉镜像
docker push 镜像
将镜像上传到docker hub上的仓库或指定仓库
docker save -o [tar文件名] [镜像名]</description></item><item><title>kubectl常用命令</title><link>https://jessestutler.github.io/posts/kubectl%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link><pubDate>Sun, 07 Mar 2021 10:30:17 +0800</pubDate><guid>https://jessestutler.github.io/posts/kubectl%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid><description>k8s命令 引言 本文列举了一些kubectl的常用命令以及其对应的常用参数
kubectl的详细信息可参照：https://kubernetes.io/docs/reference/kubectl/overview/
如果熟悉了kubectl的朋友，对于经常性的kubectl get 和kubectl describe查找resource感到繁琐，笔者在这里推荐一款开源的增强型kubectl软件k9s：https://github.com/derailed/k9s，相信对于vim熟悉的朋友会喜欢这款开源软件，而且可以当简略的dashboard使用
kubectl kubectl create -f [yaml或者json文件]
通过yaml或者json文件创建一个组件
-n 指定命名空间，如果不指定，默认是在default命名空间下，其他命令也一样
kubectl get [组件] [组件名]
获取组件的基本信息，如果想获取详细信息用kubectl describe
-o wide 显示更多信息，-o yaml 以yaml格式显示组件信息
&amp;ndash;show-labels 多显示标签
-l 标签键=值 根据标签来筛选出pod基本信息，多个键值对用逗号分隔
-L [标签名] 多显示指定标签名的标签列，多个标签用逗号分隔
&amp;ndash;all-namespaces 列出所有命名空间的组件
kubectl logs [podname]
查看pod内容器的日志输出，如果只有一个容器不用指定容器名，如果有多个容器，想查看指定容器的日志需要-c参数指定
kubectl label [组件] [组件名] key=value [--overwrite]
修改或添加组件的标签，用key=value形式，如果要复写之前的标签，需要多加一个&amp;ndash;overwrite
如果要删除之前的标签，直接在key后跟一个减号即可（即key-）
kubectl delete [组件] [组件名1] [组件名2] [...]
删除组件
-all 删除所有组件
注：删除命名空间，里面的组件也会一并删除
kubectl scale [组件] [组件名] --replicas</description></item><item><title>K8s Resource概析</title><link>https://jessestutler.github.io/posts/k8s-resource%E6%A6%82%E6%9E%90/</link><pubDate>Sun, 07 Mar 2021 10:20:30 +0800</pubDate><guid>https://jessestutler.github.io/posts/k8s-resource%E6%A6%82%E6%9E%90/</guid><description>K8s Resource概析 引言 本文参照《Kubernetes in Action中文版》及其一些网上资料，对K8s中基础的Resource进行了概析。本文会持续更新。
pod 为什么需要pod?
主要目的是由多个进程组成的一个应用程序，多个进程不能聚集在一个容器中运行**（容器的设计目的就是只运行一个进程，如果容器中运行多个不相关的进程，比如需要包含一种进程崩溃后能够重启的机制，同时将进程的活动记录记录到相同的标准输出中，我们很难确定每个进程分别记录了什么），我们用pod来封装容器，将其作为k8s的基本单位**，既可以做到一个进程单独运行于一个容器当中，容器之间相互隔离，保持了容器的特性，又能同时运行一些密切相关的进程，为他们提供相同的环境。
pod中的容器共享network namespace，容器中运行的进程之间能够通过端口来相互通信（同一个pod中的容器拥有相同的loopback网路接口，可以通过发往localhost与其他容器中的进程相互通信）
如何决定多个容器是否要放入同一个pod中？ 它们需要一起运行还是可以在不同主机上运行 它们代表的是一个整体还是相互独立的组件 它们必须一起扩缩容还是可以分别进行 liveness probe &amp;amp; readiness probe liveness probe——存活探针（在pod running时检测）
通过 TCP、HTTP 或者命令行方式对应用就绪进行检测。对于 HTTP 类型探针，Kubernetes 会定时访问该地址，如果该地址的返回码不在 200 到 400 之间，则认为该容器不健康，会杀死该容器重建新的容器。
readiness probe——就绪探针（在pod就绪前检测）
对于启动缓慢的应用，为了避免在应用启动完成之前将流量导入。Kubernetes 支持业务容器提供一个 readiness 探针，对于 HTTP 类型探针，Kubernetes 会定时访问该地址，如果该地址的返回码不在 200 到 400 之间，则认为该容器无法对外提供服务，不会把请求调度到该容器。
容器重启策略 Always ： 容器失效时，kubelet 自动重启该容器（就算成功执行完容器也会重启） OnFailure ： 容器终止运行且退出码不为0时重启 Never ： 不论状态为何， kubelet 都不重启该容器 节点亲和性 nodeAffinity:</description></item></channel></rss>