<!doctype html><html><head><title>K8s Resource概析</title><meta charset=utf-8><meta name=X-UA-Compatible content="IE=edge"><meta name=google-site-verification content><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name=viewport><meta content="telephone=no" name=format-detection><meta name=description content="对K8s基础Resource的概析"><meta name=renderer content="webkit"><meta name=theme-color content="#ffffff"><meta property="og:title" content="K8s Resource概析"><meta property="og:description" content="对K8s基础Resource的概析"><meta property="og:type" content="article"><meta property="og:url" content="https://jessestutler.github.io/posts/k8s-resource%E6%A6%82%E6%9E%90/"><meta property="og:image" content="https://jessestutler.github.io/images/k8s-logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-03-07T10:20:30+08:00"><meta property="article:modified_time" content="2021-03-07T10:20:30+08:00"><meta property="og:site_name" content="My Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jessestutler.github.io/images/k8s-logo.png"><meta name=twitter:title content="K8s Resource概析"><meta name=twitter:description content="对K8s基础Resource的概析"><script src=/vendor/js/jquery.min.js></script>
<script src=/vendor/js/popper.min.js></script>
<script src=/vendor/js/bootstrap.min.js></script>
<script src=/vendor/js/smooth-scroll.polyfills.min.js></script>
<link type=text/css rel=stylesheet href=/vendor/css/bootstrap.min.css><script src=/vendor/js/vue.min.js></script>
<link rel=stylesheet href=https://jessestutler.github.io/scss/journal.min.c116bc90d171283f099f173854157ec8f183f9073b93443b2c8ad82899ee9025.css integrity="sha256-wRa8kNFxKD8Jnxc4VBV+yPGD+Qc7k0Q7LIrYKJnukCU=" media=screen><link rel=stylesheet href=https://jessestutler.github.io/scss/dark-mode.min.552aae4638a84aa57cf0b195750a49ea9131d3bb621d1ed3ebc9b14b18166536.css integrity="sha256-VSquRjioSqV88LGVdQpJ6pEx07tiHR7T68mxSxgWZTY=" media=screen><script src=https://jessestutler.github.io//js/loadCSS.js></script>
<script>loadCSS("https://fonts.googleapis.com/css?family=Lora|Montserrat|Fira+Mono|Noto+Serif+SC|Material+Icons")</script><script src=https://jessestutler.github.io//js/toc-collapse.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script>
<script src=/vendor/js/md5.min.js></script>
<script>var gitalk=new Gitalk({clientID:"your client id",clientSecret:"your client secret",repo:"repo name",owner:"user",admin:["user"],id:md5(location.pathname),distractionFreeMode:"false"});window.onload=function(){gitalk.render("gitalk-container")}</script></head><body><div id=app><div ref=sideContainer class=side-container><a class="a-block nav-head false" href=https://jessestutler.github.io/><div class=nav-title>Jesse's Blog</div><div class=nav-subtitle>Stay Hungry,Stay Foolish.</div></a><div class=nav-link-list><a class="a-block nav-link-item active" href=/posts>归档</a>
<a class="a-block nav-link-item false" href=/categories>分类</a>
<a class="a-block nav-link-item false" href=/tags>标签</a></div><div class=nav-footer>Hugo Theme <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a> by <a href=https://amazingrise.net>Rise</a><br>Ported from <a href=https://mak1t0.cc/ target=_blank rel="noreferrer noopener">Makito</a>'s <a href=https://github.com/SumiMakito/hexo-theme-journal/ target=_blank rel="noreferrer noopener">Journal.</a><br><br>&copy;
本站遵循 CC-BY-NC 4.0 协议</div></div><div ref=extraContainer class=extra-container><div class="toc animated-visibility" :class="{ invisible: scrollY <= 140 }"><div class=toc-content><center>- CATALOG -</center><ul><li><a href=#k8s-resource%e6%a6%82%e6%9e%90 onclick="onNavClick(`#k8s-resource概析-nav`)" id=k8s-resource概析-nav>K8s Resource概析</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e5%bc%95%e8%a8%80 onclick="onNavClick(`#引言-nav`)" id=引言-nav>引言</a></li><li><a href=#pod onclick="onNavClick(`#pod-nav`)" id=pod-nav>pod</a></li><ul class=collapse data-toggle=collapse><li><a href=#liveness-probe--readiness-probe onclick="onNavClick(`#liveness-probe--readiness-probe-nav`)" id=liveness-probe--readiness-probe-nav>liveness probe & readiness probe</a></li><li><a href=#%e5%ae%b9%e5%99%a8%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5 onclick="onNavClick(`#容器重启策略-nav`)" id=容器重启策略-nav>容器重启策略</a></li><li><a href=#%e8%8a%82%e7%82%b9%e4%ba%b2%e5%92%8c%e6%80%a7 onclick="onNavClick(`#节点亲和性-nav`)" id=节点亲和性-nav>节点亲和性</a></li><li><a href=#pod%e9%97%b4%e4%ba%b2%e5%92%8c%e6%80%a7 onclick="onNavClick(`#pod间亲和性-nav`)" id=pod间亲和性-nav>pod间亲和性</a></li><li><a href=#%e6%b1%a1%e7%82%b9%e5%92%8c%e5%ae%b9%e5%bf%8d%e5%ba%a6 onclick="onNavClick(`#污点和容忍度-nav`)" id=污点和容忍度-nav>污点和容忍度</a></li><li><a href=#init-container onclick="onNavClick(`#init-container-nav`)" id=init-container-nav>init container</a></li><li><a href=#%e5%ae%b9%e5%99%a8%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6 onclick="onNavClick(`#容器资源限制-nav`)" id=容器资源限制-nav>容器资源限制</a></li><ul class=collapse data-toggle=collapse><li><a href=#pod-qos onclick="onNavClick(`#pod-qos-nav`)" id=pod-qos-nav>pod QoS</a></li></ul><li><a href=#limitrange%e5%92%8cresourcequota onclick="onNavClick(`#limitrange和resourcequota-nav`)" id=limitrange和resourcequota-nav>LimitRange和ResourceQuota</a></li><li><a href=#hpahorizontal-pod-autoscaler onclick="onNavClick(`#hpahorizontal-pod-autoscaler-nav`)" id=hpahorizontal-pod-autoscaler-nav>HPA(horizontal pod autoscaler)</a></li><ul class=collapse data-toggle=collapse><li><a href=#pod%e6%95%b0%e9%87%8f%e8%ae%a1%e7%ae%97%e6%96%b9%e5%bc%8f onclick="onNavClick(`#pod数量计算方式-nav`)" id=pod数量计算方式-nav>pod数量计算方式</a></li></ul><li><a href=#%e9%ab%98%e7%ba%a7%e8%b0%83%e5%ba%a6 onclick="onNavClick(`#高级调度-nav`)" id=高级调度-nav>高级调度</a></li><ul class=collapse data-toggle=collapse><li><a href=#priorityclass onclick="onNavClick(`#priorityclass-nav`)" id=priorityclass-nav>PriorityClass</a></li><li><a href=#pod%e6%89%93%e6%95%a3 onclick="onNavClick(`#pod打散-nav`)" id=pod打散-nav>Pod打散</a></li></ul><li><a href=#pod%e8%b0%83%e5%ba%a6 onclick="onNavClick(`#pod调度-nav`)" id=pod调度-nav>pod调度</a></li><li><a href=#pod%e5%88%a0%e9%99%a4 onclick="onNavClick(`#pod删除-nav`)" id=pod删除-nav>pod删除</a></li></ul><li><a href=#replicationcontroller onclick="onNavClick(`#replicationcontroller-nav`)" id=replicationcontroller-nav>ReplicationController</a></li><li><a href=#replicaset onclick="onNavClick(`#replicaset-nav`)" id=replicaset-nav>ReplicaSet</a></li><li><a href=#daemonset onclick="onNavClick(`#daemonset-nav`)" id=daemonset-nav>DaemonSet</a></li><li><a href=#job--cronjob onclick="onNavClick(`#job--cronjob-nav`)" id=job--cronjob-nav>Job & CronJob</a></li><li><a href=#service--endpoints onclick="onNavClick(`#service--endpoints-nav`)" id=service--endpoints-nav>Service & EndPoints</a></li><li><a href=#volume onclick="onNavClick(`#volume-nav`)" id=volume-nav>volume</a></li><ul class=collapse data-toggle=collapse><li><a href=#volume%e7%9a%84%e7%a7%8d%e7%b1%bb onclick="onNavClick(`#volume的种类-nav`)" id=volume的种类-nav>volume的种类</a></li></ul><li><a href=#configmap onclick="onNavClick(`#configmap-nav`)" id=configmap-nav>ConfigMap</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e5%8f%82%e6%95%b0%e5%bd%a2%e5%bc%8f onclick="onNavClick(`#参数形式-nav`)" id=参数形式-nav>参数形式：</a></li><li><a href=#configmap%e7%9a%84%e8%af%bb%e5%8f%96%e5%bd%a2%e5%bc%8f onclick="onNavClick(`#configmap的读取形式-nav`)" id=configmap的读取形式-nav>configmap的读取形式：</a></li></ul><li><a href=#secret onclick="onNavClick(`#secret-nav`)" id=secret-nav>Secret</a></li><li><a href=#downwardapi onclick="onNavClick(`#downwardapi-nav`)" id=downwardapi-nav>DownwardAPI</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e6%9a%b4%e9%9c%b2%e6%96%b9%e5%bc%8f onclick="onNavClick(`#暴露方式-nav`)" id=暴露方式-nav>暴露方式：</a></li></ul><li><a href=#deployment onclick="onNavClick(`#deployment-nav`)" id=deployment-nav>Deployment</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e5%8f%91%e5%b8%83%e6%96%b9%e5%bc%8f onclick="onNavClick(`#发布方式-nav`)" id=发布方式-nav>发布方式</a></li><li><a href=#kubectl-rollout-%e5%91%bd%e4%bb%a4 onclick="onNavClick(`#kubectl-rollout-命令-nav`)" id=kubectl-rollout-命令-nav>kubectl rollout 命令</a></li><li><a href=#deployment-manifest%e9%87%8d%e8%a6%81%e5%ad%97%e6%ae%b5 onclick="onNavClick(`#deployment-manifest重要字段-nav`)" id=deployment-manifest重要字段-nav>deployment manifest重要字段</a></li></ul><li><a href=#statefulset onclick="onNavClick(`#statefulset-nav`)" id=statefulset-nav>StatefulSet</a></li><li><a href=#serviceaccount onclick="onNavClick(`#serviceaccount-nav`)" id=serviceaccount-nav>ServiceAccount</a></li><ul class=collapse data-toggle=collapse><li><a href=#rbac onclick="onNavClick(`#rbac-nav`)" id=rbac-nav>RBAC</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e6%9f%90%e4%b8%aa%e5%85%b7%e4%bd%93%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4 onclick="onNavClick(`#某个具体命名空间-nav`)" id=某个具体命名空间-nav>某个具体命名空间</a></li><li><a href=#%e9%9b%86%e7%be%a4%e8%8c%83%e5%9b%b4%e6%88%96%e4%bb%bb%e6%84%8f%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4 onclick="onNavClick(`#集群范围或任意命名空间-nav`)" id=集群范围或任意命名空间-nav>集群范围或任意命名空间</a></li></ul></div></div><div class=pagination><a id=globalBackToTop class="pagination-action animated-visibility" href=#top :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up</i></a>
<a class=pagination-action v-on:click=toggleDarkMode><i class="material-icons pagination-action-icon" v-if=isDarkMode>brightness_4</i>
<i class="material-icons pagination-action-icon" v-else=isDarkMode>brightness_7</i></a></div></div><div class=single-column-drawer-container ref=drawer v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class=drawer-content><div class=drawer-menu><a class="a-block drawer-menu-item active" href=/posts>归档</a>
<a class="a-block drawer-menu-item false" href=/categories>分类</a>
<a class="a-block drawer-menu-item false" href=/tags>标签</a><div class=toc><div class=toc-content><center>- CATALOG -</center><ul><li><a href=#k8s-resource%e6%a6%82%e6%9e%90 onclick="onNavClick(`#k8s-resource概析-nav`)" id=k8s-resource概析-nav>K8s Resource概析</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e5%bc%95%e8%a8%80 onclick="onNavClick(`#引言-nav`)" id=引言-nav>引言</a></li><li><a href=#pod onclick="onNavClick(`#pod-nav`)" id=pod-nav>pod</a></li><ul class=collapse data-toggle=collapse><li><a href=#liveness-probe--readiness-probe onclick="onNavClick(`#liveness-probe--readiness-probe-nav`)" id=liveness-probe--readiness-probe-nav>liveness probe & readiness probe</a></li><li><a href=#%e5%ae%b9%e5%99%a8%e9%87%8d%e5%90%af%e7%ad%96%e7%95%a5 onclick="onNavClick(`#容器重启策略-nav`)" id=容器重启策略-nav>容器重启策略</a></li><li><a href=#%e8%8a%82%e7%82%b9%e4%ba%b2%e5%92%8c%e6%80%a7 onclick="onNavClick(`#节点亲和性-nav`)" id=节点亲和性-nav>节点亲和性</a></li><li><a href=#pod%e9%97%b4%e4%ba%b2%e5%92%8c%e6%80%a7 onclick="onNavClick(`#pod间亲和性-nav`)" id=pod间亲和性-nav>pod间亲和性</a></li><li><a href=#%e6%b1%a1%e7%82%b9%e5%92%8c%e5%ae%b9%e5%bf%8d%e5%ba%a6 onclick="onNavClick(`#污点和容忍度-nav`)" id=污点和容忍度-nav>污点和容忍度</a></li><li><a href=#init-container onclick="onNavClick(`#init-container-nav`)" id=init-container-nav>init container</a></li><li><a href=#%e5%ae%b9%e5%99%a8%e8%b5%84%e6%ba%90%e9%99%90%e5%88%b6 onclick="onNavClick(`#容器资源限制-nav`)" id=容器资源限制-nav>容器资源限制</a></li><ul class=collapse data-toggle=collapse><li><a href=#pod-qos onclick="onNavClick(`#pod-qos-nav`)" id=pod-qos-nav>pod QoS</a></li></ul><li><a href=#limitrange%e5%92%8cresourcequota onclick="onNavClick(`#limitrange和resourcequota-nav`)" id=limitrange和resourcequota-nav>LimitRange和ResourceQuota</a></li><li><a href=#hpahorizontal-pod-autoscaler onclick="onNavClick(`#hpahorizontal-pod-autoscaler-nav`)" id=hpahorizontal-pod-autoscaler-nav>HPA(horizontal pod autoscaler)</a></li><ul class=collapse data-toggle=collapse><li><a href=#pod%e6%95%b0%e9%87%8f%e8%ae%a1%e7%ae%97%e6%96%b9%e5%bc%8f onclick="onNavClick(`#pod数量计算方式-nav`)" id=pod数量计算方式-nav>pod数量计算方式</a></li></ul><li><a href=#%e9%ab%98%e7%ba%a7%e8%b0%83%e5%ba%a6 onclick="onNavClick(`#高级调度-nav`)" id=高级调度-nav>高级调度</a></li><ul class=collapse data-toggle=collapse><li><a href=#priorityclass onclick="onNavClick(`#priorityclass-nav`)" id=priorityclass-nav>PriorityClass</a></li><li><a href=#pod%e6%89%93%e6%95%a3 onclick="onNavClick(`#pod打散-nav`)" id=pod打散-nav>Pod打散</a></li></ul><li><a href=#pod%e8%b0%83%e5%ba%a6 onclick="onNavClick(`#pod调度-nav`)" id=pod调度-nav>pod调度</a></li><li><a href=#pod%e5%88%a0%e9%99%a4 onclick="onNavClick(`#pod删除-nav`)" id=pod删除-nav>pod删除</a></li></ul><li><a href=#replicationcontroller onclick="onNavClick(`#replicationcontroller-nav`)" id=replicationcontroller-nav>ReplicationController</a></li><li><a href=#replicaset onclick="onNavClick(`#replicaset-nav`)" id=replicaset-nav>ReplicaSet</a></li><li><a href=#daemonset onclick="onNavClick(`#daemonset-nav`)" id=daemonset-nav>DaemonSet</a></li><li><a href=#job--cronjob onclick="onNavClick(`#job--cronjob-nav`)" id=job--cronjob-nav>Job & CronJob</a></li><li><a href=#service--endpoints onclick="onNavClick(`#service--endpoints-nav`)" id=service--endpoints-nav>Service & EndPoints</a></li><li><a href=#volume onclick="onNavClick(`#volume-nav`)" id=volume-nav>volume</a></li><ul class=collapse data-toggle=collapse><li><a href=#volume%e7%9a%84%e7%a7%8d%e7%b1%bb onclick="onNavClick(`#volume的种类-nav`)" id=volume的种类-nav>volume的种类</a></li></ul><li><a href=#configmap onclick="onNavClick(`#configmap-nav`)" id=configmap-nav>ConfigMap</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e5%8f%82%e6%95%b0%e5%bd%a2%e5%bc%8f onclick="onNavClick(`#参数形式-nav`)" id=参数形式-nav>参数形式：</a></li><li><a href=#configmap%e7%9a%84%e8%af%bb%e5%8f%96%e5%bd%a2%e5%bc%8f onclick="onNavClick(`#configmap的读取形式-nav`)" id=configmap的读取形式-nav>configmap的读取形式：</a></li></ul><li><a href=#secret onclick="onNavClick(`#secret-nav`)" id=secret-nav>Secret</a></li><li><a href=#downwardapi onclick="onNavClick(`#downwardapi-nav`)" id=downwardapi-nav>DownwardAPI</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e6%9a%b4%e9%9c%b2%e6%96%b9%e5%bc%8f onclick="onNavClick(`#暴露方式-nav`)" id=暴露方式-nav>暴露方式：</a></li></ul><li><a href=#deployment onclick="onNavClick(`#deployment-nav`)" id=deployment-nav>Deployment</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e5%8f%91%e5%b8%83%e6%96%b9%e5%bc%8f onclick="onNavClick(`#发布方式-nav`)" id=发布方式-nav>发布方式</a></li><li><a href=#kubectl-rollout-%e5%91%bd%e4%bb%a4 onclick="onNavClick(`#kubectl-rollout-命令-nav`)" id=kubectl-rollout-命令-nav>kubectl rollout 命令</a></li><li><a href=#deployment-manifest%e9%87%8d%e8%a6%81%e5%ad%97%e6%ae%b5 onclick="onNavClick(`#deployment-manifest重要字段-nav`)" id=deployment-manifest重要字段-nav>deployment manifest重要字段</a></li></ul><li><a href=#statefulset onclick="onNavClick(`#statefulset-nav`)" id=statefulset-nav>StatefulSet</a></li><li><a href=#serviceaccount onclick="onNavClick(`#serviceaccount-nav`)" id=serviceaccount-nav>ServiceAccount</a></li><ul class=collapse data-toggle=collapse><li><a href=#rbac onclick="onNavClick(`#rbac-nav`)" id=rbac-nav>RBAC</a></li><ul class=collapse data-toggle=collapse><li><a href=#%e6%9f%90%e4%b8%aa%e5%85%b7%e4%bd%93%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4 onclick="onNavClick(`#某个具体命名空间-nav`)" id=某个具体命名空间-nav>某个具体命名空间</a></li><li><a href=#%e9%9b%86%e7%be%a4%e8%8c%83%e5%9b%b4%e6%88%96%e4%bb%bb%e6%84%8f%e5%91%bd%e5%90%8d%e7%a9%ba%e9%97%b4 onclick="onNavClick(`#集群范围或任意命名空间-nav`)" id=集群范围或任意命名空间-nav>集群范围或任意命名空间</a></li></ul></div></div></div></div></div><transition name=fade><div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if=isDrawerOpen v-on:click=toggleDrawer></div></transition><nav ref=navBar class="navbar sticky-top navbar-light single-column-nav-container"><div ref=navBackground class=nav-background></div><div class="container container-narrow nav-content"><button id=nav_dropdown_btn class=nav-dropdown-toggle type=button v-on:click=toggleDrawer>
<i class=material-icons>menu</i></button>
<a ref=navTitle class=navbar-brand href=https://jessestutler.github.io/>Jesse's Blog</a>
<button type=button class=nav-darkmode-toggle v-on:click=toggleDarkMode>
<i class=material-icons v-if=isDarkMode>brightness_4</i>
<i class=material-icons v-else=isDarkMode>brightness_7</i></button></div></nav><div class=single-column-header-container ref=pageHead v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href=https://jessestutler.github.io/><div class=single-column-header-title>Jesse's Blog</div><div class=single-column-header-subtitle>Stay Hungry,Stay Foolish.</div></a></div><div id=content><div ref=streamContainer class=stream-container><div class="post-list-container post-list-container-shadow"><div class=post><div class=post-head-wrapper style=background-image:url(https://jessestutler.github.io/images/k8s-logo.png)><div class=post-title>K8s Resource概析<div class=post-subtitle>对K8s基础Resource的概析</div><div class=post-meta><time itemprop=datePublished>2021-03-07 10:20</time>
<i class=material-icons>folder</i>
<a href=/categories/k8s>k8s</a>
&nbsp;
<i class=material-icons>label</i>
<a href=/tags/k8s>k8s</a>
&nbsp;</div></div></div><div class=post-body-wrapper><div class=post-body v-pre><h1 id=k8s-resource概析>K8s Resource概析</h1><h2 id=引言>引言</h2><p>本文参照《Kubernetes in Action中文版》及其一些网上资料，对K8s中基础的Resource进行了概析。本文会持续更新。</p><h2 id=pod>pod</h2><p>pod中的容器共享network namespace，容器中运行的进程之间能够通过端口来相互通信（同一个pod中的容器拥有相同的loopback网路接口，可以通过发往localhost与其他容器中的进程相互通信）</p><ul><li>如何决定多个容器是否要放入同一个pod中？<ul><li>它们需要一起运行还是可以在不同主机上运行</li><li>它们代表的是一个整体还是相互独立的组件</li><li>它们必须一起扩缩容还是可以分别进行</li></ul></li></ul><h3 id=liveness-probe--readiness-probe>liveness probe & readiness probe</h3><ul><li><p>liveness probe——存活探针（<strong>在pod running时检测</strong>）</p><ul><li>在容器内部执行一个命令，若该命令的退出状态码为0，则健康</li><li>通过容器的ip和端口进行TCP检查，若端口能被访问到，则容器健康</li><li>调用http get方法，若响应码在200到400之间，则健康</li></ul></li><li><p>readiness probe——就绪探针（<strong>在pod就绪前检测</strong>）</p><p>对于启动缓慢的应用，为了避免在应用启动完成之前将流量导入。Kubernetes 支持业务容器提供一个 readiness 探针，探测规则同存活探针</p></li></ul><h3 id=容器重启策略>容器重启策略</h3><ul><li>Always ： 容器失效时，kubelet 自动重启该容器（<strong>就算成功执行完容器也会重启</strong>）</li><li>OnFailure ： 容器终止运行且退出码不为0时重启</li><li>Never ： 不论状态为何， kubelet 都不重启该容器</li></ul><h3 id=节点亲和性>节点亲和性</h3><ul><li><p>nodeAffinity:</p><p>约束pod可以调度到哪些节点，是对nodeSelector的一种加强。有以下两种字段：</p><ul><li><p>requiredDuringSchedulingIgnoredDuringExecution（硬限制）：</p><p>表示node<strong>必须</strong>满足<strong>pod指定条件</strong>(matchExpression）才能将pod调度到这些节点上，否则<strong>不进行调度</strong>（与nodeSelector相同）</p></li><li><p>preferredDuringSchedulingIgnoredDuringExecution（软限制）：</p><p><strong>偏好</strong>将pod调度到满足pod指定条件的node上，如果没有满足指定条件的节点没关系，<strong>还是可以将pod调度到其他节点上</strong></p></li></ul><p><strong>IgnoredDuringExecution</strong>表示纵使节点标签发生改变，pod仍可以在节点上继续运行</p></li></ul><h3 id=pod间亲和性>pod间亲和性</h3><p><strong>如果是大集群不建议使用pod亲和，否则需要涉及到大量的处理</strong></p><ul><li><p>podAffinity和podAntiAffinity（pod亲和和反亲和）:</p><p><strong>既需要node匹配，又需要pod匹配</strong>，他的规则是：如果 X 节点上已经运行了一个或多个 满足规则 Y 的pod，则这个 pod 应该（或者在非亲和的情况下不应该）运行在 X 节点，X和Y都是标签（X是node的标签，可以是k8s内置标签也可以是自定义标签，表示一个<strong>拓扑域</strong>，Y是pod标签）</p><p>字段与nodeAffinity相同，需要在pod的Spec中写明</p></li></ul><h3 id=污点和容忍度>污点和容忍度</h3><p>污点其实就是节点的<strong>反亲和性</strong>，用处在于某些pod需要调度到特定节点上，而其他pod不能调度到这些节点上（就需要给pod加上容忍度），或者是某些节点挂掉了，需要驱逐某些pod，或者加上容忍度，容忍在指定时间内节点可以恢复，否则就要被驱逐</p><p>使用 kubectl taint nodes node1 key=value:NoSchedule命令给node加上taint，用法类似kubectl label，要说明的是NoSchedule，有以下几种动作：</p><ul><li>NoSchedule，如果node打上了taint，且pod没有指明tolerations，则pod不会被调度到该节点上</li><li>NoExecute，如果node打上了taint，则在该节点上运行的pod会被驱逐（一般是pod已经在该node上运行了，然后node挂了，由k8s自动给该node打上taint，然后驱逐node上运行的pod），pod也可以指明tolerations不被驱逐，或者在tolerations当中指明容忍时间，在超出容忍时间之后仍会被驱逐</li><li>PreferNoSchedule，如果没有合适的node，则可以被调度该node上（软限制）</li></ul><p>节点可能会出现某种问题，如not-ready或者unreachable，那么<strong>k8s会自动给node打上以下taint：</strong></p><p>如果pod未指定对以下两种taint的容忍度，则k8s自动会给pod加上对以下taint的tolerations并指定容忍时间为300s，超过该时间taint未清除则pod仍会被驱逐</p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEgy1goa6tpwjs7j314i05sgms.jpg alt=image-20201106201619457></p><h3 id=init-container>init container</h3><p>init container是pod中先于应用容器创建的容器，用于为应用容器进行一些初始化配置（配置卷，配置环境变量等等），或者安装一些实用工具等等，<strong>推荐image使用busybox，然后再运行一些命令</strong></p><p><strong>init container可以有多个，顺序执行，前一个成功执行后才能执行后一个（如果init容器执行失败，则k8s会重启该pod），有点类似job，全部成功执行完成之后（注意是之后），才会创建应用容器</strong></p><p>init container不能使用readinessProbe和livenessProbe，<strong>因为他们在应用容器创建之前完成，pod还未就绪</strong></p><h3 id=容器资源限制>容器资源限制</h3><p>除自定义资源限制外，一般限制CPU和memory</p><p>两种限制：</p><ul><li><p>requests（CPU shares）:</p><p><strong>节点上pod的requests总和不能超过节点可用资源的100%</strong>，k8s的调度器调度也是根据requests来调度，如果pod的requests<strong>小于</strong>（节点可使用资源总量-其他pod申请量），即使其他pod的<strong>实际使用资源量</strong>未达到他们所申请量，这个pod也不可以调度到这个节点上。当未进行limits限制时，pod的实际使用资源量是可以<strong>超过</strong>申请量的，比如有pod暂时不需要CPU时，而有计算密集型pod需要全力使用CPU时，便可以超过申请量全部占用CPU。</p></li><li><p>limits（CPU quota + CPU period):</p><p><strong>节点上pod的limits总和可以超过节点可用资源的100%，我们称之为超卖</strong>，CPU进行limits限制时，进程分不到比限制量更多的CPU，而内存与CPU不同，因为内存一旦分配给进程，想要回收就需要进程主动释放，<strong>所以如果有恶意pod或者故障pod</strong>，或者是贪婪的进程就有可能吃掉节点上所有的内存，需要进行限制，<strong>进程如果申请超过limits的量</strong>或者节点使用总量超过100%就有可能会造成OOM(out of memory)，进程就会根据策略被杀死（linux OOM_reaper subsystem）</p></li></ul><h4 id=pod-qos>pod QoS</h4><p>三种等级（优先级从低到高）：</p><ul><li>BestEffort（未设置limit和request）</li><li>Burstable（limit和request设置不相等）</li><li>Guranteed（limit和request设置相等）</li></ul><p>当资源不够时，比如节点内存不够用时，就需要杀死pod，根据优先级<strong>从低到高来杀死</strong></p><p>那怎么来决定pod的QoS class呢，粗略的规则就是，BestEffort未声明requests和limits；Guranteed必须声明requests和limits，且两者相等；其他情况皆为Burstable（比如只有requests或者limits，requests&lt;limits等），细致的规则可以参考：</p><p><strong>单容器pod</strong>：</p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEly1gnu8y3ll3pj32nb0u0qv5.jpg alt=img></p><p><strong>多容器pod:</strong></p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEly1gnu8yai41bj32m20u0qv5.jpg alt=img></p><p><strong>当资源不够时，不同等级的情况下先杀死QoS等级低的Pod，相同等级的pod根据已使用资源量和可用资源量的比较来杀死</strong></p><h3 id=limitrange和resourcequota>LimitRange和ResourceQuota</h3><p>LimitRange是用来限制某个命名空间<strong>单个pod</strong>的可用资源，API server中存在LimitRange admission control插件，当发起Pod Post请求时就需要进行检查或者添加字段，比如pod层面可以限定pod中所有容器的最小requests总和或最大limits总量，容器层面可以限定默认的requests和默认的limits，还可以限定PVC最大申请量等。不同命名空间可以有不同的LimitRange，但是LimitRange无法限制命名空间中所有pod的资源总量，这需要ResourceQuota</p><p>ResourceQuota是用来限制某个命名空间<strong>所有pod</strong>的可用资源，限定命名空间中所有pod requests总和或者limits总和，但是要<strong>注意的是</strong>，这样pod必须限定resources字段，不然ResourceQuota无法确定已用的requests和limits量，或者明确给命名空间声明一个LimitRange</p><h3 id=hpahorizontal-pod-autoscaler>HPA(horizontal pod autoscaler)</h3><p>Autoscaler根据指定资源（通常是<strong>CPU或者QPS（每秒查询率）<strong>等）的metrics来</strong>自动水平扩缩</strong>pod（deployment或statefulset等资源控制的pod），<strong>其通过根据从metric server采集到的pod的metrics来进行计算，从而改变deployment等资源的replicas</strong>，达到自动扩缩pod的目的</p><p>不过要注意的是，这个要让指定资源扩缩到的目标值是需要<strong>显式</strong>的让管理员指定的，这个只能根据经验来确定</p><h4 id=pod数量计算方式>pod数量计算方式</h4><ul><li>单metric：如果是单资源计算的话，根据pod的<code>[实际使用资源量之和/目标值]</code>再向上取整</li><li>多metrics：多资源计算的话，取各单metric的计算出的数量的Max值</li></ul><h3 id=高级调度>高级调度</h3><h4 id=priorityclass>PriorityClass</h4><p>PriorityClass也是一种resource，用户可以指定value为0-1000000000之间的优先级，然后通过在Pod spec中指定PriorityClassName。优先级越高的Pod，在<strong>多个Pod同时在队列中等待调度时可以优先被调度</strong>，而且如果存在节点资源不够的情况下，<strong>优先级低的Pod可以被优先级高的Pod抢占，优先级低的Pod被驱逐而等待重新调度</strong></p><h4 id=pod打散>Pod打散</h4><p><a href=https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/>https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/</a></p><p>将Pod均匀分布到指定拓扑域当中，这个拓扑域可以是Region,Zone或者Node。使各拓扑域中的Pod数量均匀分布。</p><h3 id=pod调度>pod调度</h3><p>两个主要队列：activeQ,unschedulableQ，需要调度的pod加入activeQ，调度失败的pod加入unschedulableQ</p><ul><li><p>过滤阶段</p><p>比如筛选可用nodes，比如node资源是否足够，pod指定的卷或者端口是否可用，是否指定了node或者可容忍taint等</p></li><li><p>打分阶段</p><p>有pod亲和性，node亲和性打分，还有主要的根据节点水位打分：</p><ul><li>优先打散，就是根据 （节点可分配量-pod请求量）/节点可分配量，优先选择空闲资源比例最高的节点</li><li>优先堆叠，根据 已分配量/节点可分配量，优先选择负载高的节点</li><li>根据碎片率，按照CPU和内存两种资源来说，碎片率=1-Abs[CPU(Request / Allocatable) - Mem(Request / Allocatable)]</li></ul></li></ul><h3 id=pod删除>pod删除</h3><p>pod删除并不是立即删除，默认设置的grace-period是30秒，允许pod进行一些清理工作之后退出。</p><ul><li>pod被标记为Terminating状态</li><li>如果pod有preStop回调函数被设置，则执行，允许执行回调函数时间超过一点grace-period</li><li>发送SIGTERM信号给pod中的所有容器的进程1</li><li>pod从service的endpoints，replicaset等管理中移除</li><li>超过grace-period仍未删除，则触发强制删除，发送SIGKILL信号给所有容器中的进程</li></ul><h2 id=replicationcontroller>ReplicationController</h2><p>rc负责创建和管理pod的多个副本，其有三个部件：</p><ul><li><p>label selector（标签选择器）：</p><p>用来标识pod，rc<strong>只负责管理创建rc时指定标签的pod</strong>（<strong>只管自己家的孩子</strong>），如果在这之后pod更改了标签，就脱离了rc的管理范围（pod相当于手动创建，无人管理，变成了孤儿），rc认为现有pod数与replicas不符合（即少了一个pod），会根据template创建新的pod</p></li><li><p>replica count（副本个数）：</p><p>指定pod副本数</p></li><li><p>pod template（pod模板）：</p><p>用来创建新的pod，之后可以更改pod模板，只是创建出来的pod和之前的不一样了，rc并不关心pod长什么样，它只关心pod的副本数是不是符合预期</p></li></ul><p><strong>如果pod数多了rc就会删除多余的pod（使符合replicas数），少了就会创建新的pod</strong></p><p>可以用kubectl scale改变副本数量来水平扩缩pod</p><h2 id=replicaset>ReplicaSet</h2><p>ReplicationController的替代品，具有更强的标签筛选功能，工作机理与rc相同，<strong>以后用rs即可</strong></p><p>可基于标签名匹配，多标签匹配，单标签多值匹配，甚至匹配缺少某个标签的pod等（增加operator为in，notin，exists等等）</p><h2 id=daemonset>DaemonSet</h2><p>如果不指定selector，就会在<strong>每个节点上管理一个pod（这些pod常用作来日志收集和资源监控）</strong>，指定了对于node标签的selector，就只会在特定节点上每个节点管理一个pod</p><h2 id=job--cronjob>Job & CronJob</h2><p><strong>job是一个批处理任务</strong>（可顺序可并行），用来安排一个或多个运行完后退出的pod</p><p>注意：</p><ul><li><p>如果需要多次完成，可以指定completions字段，表示需要完成几次任务，job跑完一个pod后会再创建一个pod，以此类推（顺序执行），也可以指定parallelism数量并行执行</p></li><li><p><strong>job中pod的restartPolicy必须为onFailure或者Never</strong>，如果是onFailure，则容器异常退出后，pod仍会在原node上，而重启容器；如果是Never，则pod failed时，job会新建一个pod</p></li><li><p><strong>job存在运行时限，如果超过了activeDeadlineSeconds这个时限，则job会视为失败</strong></p></li></ul><p><strong>一般创建cronjob来创建job</strong>，从而来创建pod，cronjob机制与linux crontab相同，会定时执行job</p><h2 id=service--endpoints>Service & EndPoints</h2><p>将相同功能的pod组（通过标签筛选）暴露一个固定的IP给外部或者内部集群访问（通过端口转发），<strong>这样pod即使销毁或者转移，固定IP也不会变动</strong>，当有client请求service时，service会随机选择一个pod进行转发</p><p>参考资料：https://www.qikqiak.com/post/visually-explained-k8s-service/</p><ul><li><p>服务发现：</p><ul><li>service如果早于pod创建，k8s会将为service维护的一些环境变量配置到容器当中</li><li>service也能通过kube dns通过域名解析来找到对应的ip（直接访问服务的FQDN，所有service对应的pod的容器当中，/etc/resolv.conf都有域名配置）</li></ul></li><li><p>Endpoints：</p><p>Endpoints是<strong>一对对的 ip：端口 对</strong>（容器对外提供服务的那个端口），创建服务时是根据标签筛选器获得对应pod的ip：端口对，然后存储在Endpoints当中，当客户端连接service时，服务代理选择Endpoints当中的一个ip端口对，进行请求转发，也就是说，<strong>服务有了Endpoints，才能将请求转发到相应的pod，没有标签筛选器筛选出对应的pod，服务只是个空服务而已</strong>，如果更换或者移除了选择器，Endpoints也会跟着变动或者停止更新</p></li><li><p>将服务暴露给客户端：</p><ul><li><p>Nodeport:将service的type设置为nodeport，k8s将会为每个节点上开一个指定端口，到该端口的流量都会重定向到此服务，然后再由服务重定向到pod</p><p>Client &ndash;> 节点的指定端口 &ndash;>cluster服务&ndash;> pod</p><p>既可以访问服务的集群ip来访问，也可以访问节点指定端口来访问</p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEgy1gob3wn28e1j31400u0e8d.jpg alt=image-20200917195909101></p></li></ul></li><li><p>LoadBalancer：将service的type指定为LoadBalancer，LoadBalancer需要有独有的公网ip，专门由负责负载均衡的机子来做重定向， 机制与nodeport一致（<strong>只不过多做了一次数据流重定向</strong>）</p><p>Client &ndash;> LoadBalancer &ndash;> 节点的指定端口&ndash;>cluster服务&ndash;> pod</p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEgy1gob3wrz939j31400u0b2k.jpg alt=image-20200917200648198></p><p>​ 总结：Load Balancer &ndash;> Node Port &ndash;> cluster service 层层往下</p></li><li><p>Ingress：</p><p>通过外加的ingress controller来进行反向代理和负载均衡，当客户端向ingress发送http请求，根据主机名和路径分发到对应的服务（<strong>这样既可以只用一个公网ip就可以为多服务做负载均衡和对外暴露，又可以避免nodeport开放过多端口可能造成的安全问题，因为运行k8s需要关闭防火墙</strong>），通过与该服务相关联的Endpoint来查看ip，然后将客户端的请求转发给其中一个pod</p><p>Client &ndash;> ingress controller &ndash;> pod</p><p>Ingress controller实现方案： nginx，traefik，HAproxy等等</p><p>ingress参考资料：https://www.cnblogs.com/linuxk/p/9706720.html</p><p>ingress controller通常通过daemonset实现，在每个node上跑一个pod（如nginx），它会监听ingress的变化，一旦有新的ingress就根据模板生成配置然后写入pod中，然后reload。ingress就是转发规则，要发往哪些service，指定什么URL来转发到相应的service。</p></li><li><p>headless服务：</p><p>将服务的clusterIP字段设置为none，k8s便不会为其分配集群ip，用dns查找pod的DNS记录，dns服务器会直接返回pod ip</p><p><code>&lt;pod-name>.&lt;svc-name>.&lt;namespace>.svc.cluster.local</code></p></li><li><p>External Name:</p><p>通过external name指定集群外部服务（DNS名），将其虚拟为内部服务（直接通过cluster ip访问即可）</p></li></ul><h2 id=volume>volume</h2><p><strong>使用卷来让容器之间共享文件或者持久化文件</strong></p><h3 id=volume的种类>volume的种类</h3><p>后三者比较普遍</p><ul><li><p>emptyDir（<strong>一个pod中的多个容器共享</strong>）:</p><p>emptyDir卷的声明周期与pod的生命周期相同，卷的声明是在创建pod时声明的，<strong>pod删除卷也跟着删除</strong>，卷从一个空目录开始，挂载到多个容器的某个文件夹后（文件夹在不同容器的文件系统的位置可以不一样），多个容器之间便可以往里面存放文件，以便共享</p></li><li><p>gitRepo:</p><p>gitRepo实际上是一个emptyDir卷，与emptyDir同理，只不过创建卷后卷中会加入从git远程repo中clone下来的文件，比如web服务容器挂载了这样一个卷，网页开发者往git远程仓库里push新的网页版本，sidecar容器（通常是git syc容器）便可以进行数据同步，以便web服务器对外服务（<strong>其实就是emptyDir的基础上之后多加了git repo中clone下来的数据而已</strong>）</p></li><li><p>hostPath:</p><p>hostPath可以提供持久化存储，不会随pod删除而删除，它访问的是节点上的文件系统，容器可以通过挂载hostPath卷访问节点上的某些特定文件夹下的文件，但是<strong>不推荐使用hostPath来作为数据库数据等需要一直保存在目录上的数据</strong>，因为如果pod被重新调度到其他节点上就找不到hostPath上的数据了，所以应该用pv</p></li><li><p>configMap:</p><p>将每个configMap中的键值对暴露为文件放到卷中，挂载到容器当中之后从而容器可以进行读取，与emptyDir和gitRepo同理</p></li><li><p>PersistentVolume（持久化存储，后端存储）</p><p><strong>PV是对分布式存储资源的抽象</strong>，pv的提供者可以是各种云提供商，如AWS，GCE，也可以是nfs，glusterfs，ceph等开源分布式存储系统，具体实现具体对待。pv的信息包括存储能力（比如容量多大），访问模式（是ReadWriteOnce，ReadOnlyMany，还是ReadWriteMany），回收策略（与pvc解绑后对pv的处理是保留，回收还是删除）等。</p><p>pvc是对pv的申请，可以与pv进行一一绑定，只要有pv满足pvc中申请的容量和访问模式，k8s就会将其绑定起来，pv一旦与pvc绑定起来就不能再去与其他pvc绑定了，但是如果pv大小不满足pvc申请量，pvc会一直pending，直到有足够大小的pv加入。<strong>pvc就相当于是卷，然后pod使用其来挂载到对应目录，相当于是pod->pvc->pv三层结构</strong></p><ul><li><p>动态绑定（<strong>使用StorageClass</strong>)：</p><p>原始的静态绑定如下：</p><p><img src=https://www.kubernetes.org.cn/img/2018/06/20180604211538.png alt=img></p><p>原始静态绑定需要管理员实现创建好pv，工作是非自动的，且pv的量是静态的，很有可能造成资源浪费，所以推荐使用动态绑定：</p><p><img src=https://www.kubernetes.org.cn/img/2018/06/%E5%9B%BE%E7%89%872.png alt=img></p><p>现在加入一个资源叫StorageClass，用户创建pvc后，storageclass的控制器会根据pvc指定的storageClassName来找到对应的存储类，然后动态的创建pv，再将其与pvc绑定，这样的好处是<strong>既动态创建pv，减少了管理员的工作量，又使用存储类来抽象描述一种存储类别，比如是”快速存储“还是”慢速存储“，是”冗余存储“还是”无冗余存储“等等，非常直观</strong></p><p>如果没有默认存储类，且pvc没有指定storageClassName，则这个pvc只能与没有指定存储类的pv绑定，否则没有指定storageClassName的pvc将交由默认存储类处理</p></li></ul></li></ul><h2 id=configmap>ConfigMap</h2><p>将配置单独列为资源，方便移动和修改，<strong>这样pod定义时不用定死和重复环境变量配置</strong>，也增加了配置的重用性，而且使用环境变量和命令参数作为pod配置的方法无法在进程运行时更新配置，<strong>configmap暴露为卷可以进行热更新,无需重新创建pod或重启容器</strong></p><p><strong>configmap中存放的就是key-value对</strong></p><p>kubectl create configmap [configmap-name]即可创建configmap（或者kubectl create -f xxx.yaml）</p><h3 id=参数形式>参数形式：</h3><ul><li><p>&ndash;from-literal <em>key</em> value 直接用key*value形式定义变量</p></li><li><p>&ndash;from-file*[file or directory] 读取文件为变量，如果没指定key，则直接用file的名字为key，value为file中的数据，如果读取的是整个文件夹，则文件夹下所有的文件都会被列为key*value</p></li></ul><p><strong>&ndash;from-literal和&ndash;from-file都可以写多个</strong></p><h3 id=configmap的读取形式>configmap的读取形式：</h3><ul><li>pod的yaml中指定env或者envFrom来给容器当中的环境变量赋值，在这基础之上，还可以再传给pod中的yaml中的args来给命令参数赋值（<strong>适用于变量值较短的场景</strong>）</li><li>使用configmap卷来存放configmap中的所有键值对，然后挂载到容器当中（<strong>推荐，将变量值整合成为文件</strong>）。<strong>需要注意的是，挂载会覆盖挂载点下原已有的文件，如果有需要不覆盖的话可以用subpath指定需要挂载的某个文件加到挂载点目录当中</strong></li></ul><h2 id=secret>Secret</h2><p>原理与configmap类似，可以存放二进制数据，采用base64编码，专门用来存储敏感数据（安全），<strong>configmap用来存储非敏感数据（一些配置）</strong></p><p>用secret卷挂载到容器当中即可引用secret内的key-value键值对，<strong>最好不要用环境变量引用，因为环境变量会涉及到安全问题</strong></p><p>Tips:</p><ul><li>k8s通过将secret分发到需要访问secret的pod所在的节点的内存中，这样也方便删除</li><li>如果未给pod指定service account，k8s会给pod指定其所在namespace的default service account，会在每个容器的/var/run/secrets/kubetnetes.io/serviceaccount目录挂载一个默认secret卷:default-token-xxxx，这个secret中含三个条目，分别是：<ul><li>ca.crt：CA的证书，当pod访问API服务器时（通过https），验证API服务器返回的证书就是CA签发的</li><li>namespace: pod所在的namespace</li><li>token: 用来通过api server的认证</li></ul></li></ul><h2 id=downwardapi>DownwardAPI</h2><p>使用configmap和secret无法让应用获取到<strong>pod的ip，pod的名称，pod运行节点的名称</strong>等等只有pod成功部署后才会获取到的元数据，通过DownwardAPI的方式可以暴露一个pod这些部分元数据</p><h3 id=暴露方式>暴露方式：</h3><ul><li>直接在pod manifest的container当中声明环境变量，引用manifest当中的某些字段（环境变量方式）</li><li>通过挂载DownwardAPI卷来获取文件，从而获取元数据（卷方式，<strong>卷方式才能暴露pod的标签和注解</strong>）</li></ul><h2 id=deployment>Deployment</h2><p>deployment用于部署应用并以声明的形式升级应用</p><p>创建deployment时，replicaset也会跟着创建，由rs来管理pod，deployment管理多个rs</p><p><strong>更改deployment的</strong>pod模板<strong>即可完成升级，由k8s的控制器操作完成</strong></p><h3 id=发布方式>发布方式</h3><ul><li><p>蓝绿发布</p><p>在运行新版本的pod之前，service流量始终定位到旧版本pod，一旦确定新版本功能运行正常，修改service的选择器，定位到新版本的pod，删除旧版本的replicaset</p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEgy1gob3wyc4q2j31wu0u0u13.jpg alt=image-20201021161437006></p></li><li><p>滚动发布</p><p>新的rs控制新版本的pod，通过更改新旧rs的期望数量来弹性扩缩旧版本与新版本的pod数量，新版本pod一点点增多，旧版本pod一点点减少，直到所有更新为止</p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEgy1gob3x1q66qj31ou0u0npe.jpg alt=6D21A0ED0EAF37931D60F5EF5098258D></p></li><li><p>金丝雀发布（灰度发布）</p><p>一小部分pod升级为新版本，牺牲一小部分用户的体验（beta版本），如果新版本适用则全部升级完成，否则回滚到上个版本</p></li></ul><h3 id=kubectl-rollout-命令>kubectl rollout 命令</h3><p>进行升级deployment时，可以通过kubectl rollout进行升级查看和回滚等</p><ul><li><p>kubectl rollout status deployment [name]</p><p>查看滚动升级状态</p></li><li><p>history deployment [name]</p><p>查看升级历史（可以指定revision来回滚指定版本）</p></li><li><p>pause 暂停滚动升级</p></li><li><p>resume 恢复滚动升级</p></li><li><p>undo 回滚到上个版本</p><p>可添加&ndash;to-revision参数回到history给出的指定revision版本</p></li></ul><h3 id=deployment-manifest重要字段>deployment manifest重要字段</h3><ul><li><p>maxSurge</p><p>除deployment定义的期望值外，最多允许超过的pod可用实例数量</p></li><li><p>maxUnavailable</p><p>相对于期望值，最多不可用pod的数量</p></li></ul><p>可以通过修改maxSurge和maxUnavailable控制滚动升级速度</p><ul><li><p>minReadySeconds</p><p>pod至少要运行minReadySeconds秒，才能视其为可用状态，因为maxUnavailable限制了不可用pod数量，所以不可用的话，滚动升级不会继续</p></li></ul><h2 id=statefulset>StatefulSet</h2><p><strong>用来维护有状态应用</strong>，因为有状态应用可能需要主机名，ip等不变</p><p>statefulset维护的pod副本名是可预知的，按照statefulset名称加顺序索引值组成，pod的序号是递增的，只有在低序号的pod处于running和ready状态之后高序号的pod才会部署，缩容也是同个道理，要在高序号已经停止和删除之后才会终止低序号pod。K8S原生的statefulset不能删除[0,N-1]之间序号的某个pod</p><p>statefulset对于有状态应用做出的改变：</p><ul><li><p>对于ip可能的改变：</p><p>需要你除statefulset之外额外定义一个headless service，然后根据域名来访问各个pod，这样每个pod的dns记录固定</p></li><li><p>对于主机名维持不变</p></li><li><p>对于pod删除而存储可能发生的变动：</p><p>如果statefulset维护的pod被删除了，与replicaset相同，statefulset会重新创建pod，但这个pod可能会调度到其他节点上，对于有状态应用来说，有状态的pod应该挂载原来相同的存储，这就需要pvc和pv，创建一个statefulset的时候，不仅需要写pod模板，还需要写pvc，<strong>statefulset对每个pod都创建一个或多个独立的pvc</strong>，因为挂载的pvc不变，所以新实例读取的仍然是旧实例的数据</p><p>注意：</p><p>如果pod被意外删除，<strong>其挂载的pvc不会被删除</strong>，因为如果pvc被删除，其绑定的pv可能会被回收或者删除，那其上的数据就丢失了，所以无论是缩容还是意外删除了statefulset的pod，都会保留pvc，意外缩容了statefulset之后仍然能通过扩容，将pod重新挂载原来丢失了挂载的pvc，但是statefulset只能<strong>线性缩容</strong>，不能在有一个实例不健康的时候进行缩容，这时同时有两个pod下线，那数据就丢失了。如果想删除pv，需要<strong>手动</strong>删除绑定的pvc</p></li></ul><p>tips:</p><ul><li><p>Statefulset因为pod启动有先后关系，所以角色是根据id号定死的，比如mysql statefulset的pod-0为master节点，其他序号大于0的pod为slave节点，但是如果需要场景下（比如redis哨兵模式）master挂了要slave能够成为master，那么statefulset就不好实现，需要引入Operator（有可能是Master一直在重启，那么就一直没主节点，其他pod的序号是不会动的）</p></li><li><p>Statefulset和Daemonset都可以滚动更新，和deployment的功能相同，主要是依赖于有个api资源叫<strong>controllerRevision</strong>，保存了控制器的api声明（Yaml）的历史版本，每个版本对应一个controllerRevsion</p></li></ul><h2 id=serviceaccount>ServiceAccount</h2><p>user account是给集群外部用户设计的，而service account是为了<strong>pod访问api server</strong>而设计的（每创建一个service account就会有对应的一个secret被创建，这个secret当中内含ca.cert，namespace，token，挂载点/var/run/secrets/kubernetes.io/serviceaccount，跟secret章节中提到的default token是一样的）</p><p><strong>tips:</strong></p><p>Api server的访问过程需要经过的插件：认证(authenticaiton)&ndash;>授权(authorization)&ndash;>准入控制(admission control)</p><p>当创建一个namespace时，就会创建一个默认的default serviceaccount（每个namespace底下一个），所以service account是局限于其所在的namespace的。当创建一个pod时，默认指定的service account就是pod所在namespace底下的default（除非指定某个service account），<strong>这个service account在默认没有绑定rolebinding或者clusterrolebinding的情况下，是没有查看和修改集群资源权限的</strong></p><h3 id=rbac>RBAC</h3><p>四种RBAC资源（role可以理解为对于资源的权限，binding就是将权限赋予某个account）</p><h4 id=某个具体命名空间>某个具体命名空间</h4><ul><li>role:<strong>对仅限于某个特定命名空间</strong>的资源的操作权限</li><li>rolebinding:<strong>仅限于某个特定命名空间</strong>的权限绑定</li></ul><h4 id=集群范围或任意命名空间>集群范围或任意命名空间</h4><ul><li>clusterrole:<strong>对于集群资源或所有命名空间资源或非资源型URL</strong>的操作权限</li><li>clusterrolebinding:<strong>对于集群资源或所有命名空间资源或非资源型URL</strong>的权限绑定</li></ul><p><strong>当binding绑定到某个service account或user account之后，这个account就获得了对于role规定的权限</strong></p><p><img src=https://tva1.sinaimg.cn/large/008eGmZEly1gni8tkzq8nj32c30u04qq.jpg alt=img></p><p>一些重要的系统自定clusterrole：</p><ul><li>view：除了role，rolebinding和secret外可以读取大部分资源</li><li>edit：除了role，rolebinding外可以读取和修改大部分资源</li><li>admin: 除了ResourceQuota能读取和修改所有资源</li><li>cluster admin:完全能读取和修改所有资源</li></ul><hr width=100% id=EOF><p style=color:#777>Last modified on 2021-03-07</p></div></div><nav class=post-pagination><a class=newer-posts href=https://jessestutler.github.io/posts/kubectl%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Next<br>kubectl常用命令</a>
<a class=older-posts href=https://jessestutler.github.io/posts/k8s-controller%E6%A6%82%E6%9E%90/>Previous<br></a></nav><div class=post-comment-wrapper><div id=gitalk-container></div></div></div></div></div></div><div id=single-column-footer>Hugo Theme <a href=https://github.com/amazingrise/hugo-theme-diary>Diary</a> by <a href=https://amazingrise.net>Rise</a><br>Ported from <a href=https://mak1t0.cc/ target=_blank rel="noreferrer noopener">Makito</a>'s <a href=https://github.com/SumiMakito/hexo-theme-journal/ target=_blank rel="noreferrer noopener">Journal.</a><br><br>&copy;
本站遵循 CC-BY-NC 4.0 协议</div></div><script>let app;app=new Vue({el:"#app",data:{scrollY:0,navOpacity:0,isDrawerOpen:!1,mounted:!1,isDarkMode:!1},methods:{sgn(e,t){let n=1/(1-2*e);return t<=e?0:t>=1-e?1:n*(t-e)},handleScroll(){this.scrollY=window.scrollY,this.navOpacity=this.sgn(0,Math.min(1,Math.max(0,window.scrollY/(this.pageHeadHeight()-this.navBarHeight()*.8))));const{navBar:n,navBackground:e,navTitle:t,extraContainer:s,streamContainer:o}=this.$refs;this.navOpacity>=1?(e.style.opacity=1,t.style.opacity=1):(e.style.opacity=0,t.style.opacity=0)},handleResize(){const{navBar:n,navBackground:s,navTitle:o,extraContainer:e,streamContainer:t}=this.$refs;e.style.left=t.offsetWidth-e.offsetWidth+"px"},navBarHeight(){return this.$refs.navBar.offsetHeight},pageHeadHeight(){return this.$refs.pageHead.offsetHeight},toggleDrawer(){this.isDrawerOpen=!this.isDrawerOpen,document.getElementsByTagName("html")[0].style.overflow=this.isDrawerOpen?"hidden":"unset"},closeDrawer(){this.isDrawerOpen=!1,document.getElementsByTagName("html")[0].style.overflow=this.isDrawerOpen?"hidden":"unset"},toggleDarkMode(){this.isDarkMode=!this.isDarkMode,this.isDarkMode==!0?(document.cookie="night=1;path=/",document.body.classList.add("night")):(document.cookie="night=0;path=/",document.body.classList.remove("night"))}},created(){window.addEventListener("scroll",this.handleScroll),window.addEventListener("resize",this.handleResize),window._nonDesktop=function(){let e=!1;return function(t){(/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino|android|ipad|playbook|silk/i.test(t)||/1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw-(n|u)|c55\/|capi|ccwa|cdm-|cell|chtm|cldc|cmd-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc-s|devi|dica|dmob|do(c|p)o|ds(12|-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(-|_)|g1 u|g560|gene|gf-5|g-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd-(m|p|t)|hei-|hi(pt|ta)|hp( i|ip)|hs-c|ht(c(-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i-(20|go|ma)|i230|iac( |-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|-[a-w])|libw|lynx|m1-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|-([1-8]|c))|phil|pire|pl(ay|uc)|pn-2|po(ck|rt|se)|prox|psio|pt-g|qa-a|qc(07|12|21|32|60|-[2-7]|i-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h-|oo|p-)|sdk\/|se(c(-|0|1)|47|mc|nd|ri)|sgh-|shar|sie(-|m)|sk-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h-|v-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl-|tdg-|tel(i|m)|tim-|t-mo|to(pl|sh)|ts(70|m-|m3|m5)|tx-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas-|your|zeto|zte-/i.test(t.substr(0,4)))&&(e=!0)}(navigator.userAgent||navigator.vendor||window.opera),e};var e=document.cookie.replace(/(?:(?:^|.*;\s*)night\s*=\s*([^;]*).*$)|^.*$/,"$1");e==""?window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches:e=="1"&&this.toggleDarkMode()},mounted(){this.handleScroll(),this.handleResize(),this.mounted=!0},destroyed(){window.removeEventListener("scroll",this.handleScroll),window.removeEventListener("resize",this.handleResize)}})</script><script src=https://jessestutler.github.io//js/journal.js></script></body></html>